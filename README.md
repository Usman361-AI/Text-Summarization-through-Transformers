# ðŸ§  Text Summarisation using Transformer

This project demonstrates a **Transformer-based model** for abstractive text summarization using a toy dataset. It showcases the architecture, tokenization, positional encoding, and training of a Transformer from scratch using TensorFlow.

---

## ðŸ“Œ Project Overview

| Component         | Description |
|------------------|-------------|
| ðŸ“„ Dataset        | A toy dataset with 5 sample articles and their summaries |
| ðŸ” Preprocessing  | Text cleaning, tokenization, padding |
| ðŸ”§ Model          | Transformer-based encoder-decoder using Keras |
| ðŸ“ˆ Output         | Trained model to predict summaries from input texts |

---

## ðŸ› ï¸ Model Architecture

The model consists of:
- Token and position embeddings
- Multi-head self-attention layers
- Feed-forward neural networks
- Encoder-decoder attention
- Custom loss and training loop

---

## ðŸ“Š Training

The model is trained on a small dataset for demonstration purposes. The loss over epochs is plotted at the end.

---

## ðŸš€ Sample Prediction

> **Input**: "The stock market crashed due to inflation fears."  
> **Predicted Summary**: *"Market crashed."*

---

## ðŸ§¾ Requirements

Install all dependencies via:

```bash
pip install -r requirements.txt
```

---

## ðŸ“‚ File Structure

```
â”œâ”€â”€ Text_summarisation_Transformer.ipynb  # Main notebook
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
```

---

## ðŸ™Œ Author

This project was built as a demonstration of transformer models on simple text. Feel free to fork and extend with larger datasets!

